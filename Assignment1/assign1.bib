@article{syntacticNGrams,
title = {Syntactic N-grams as machine learning features for natural language processing},
journal = {Expert Systems with Applications},
volume = {41},
number = {3},
pages = {853-860},
year = {2014},
note = {Methods and Applications of Artificial and Computational Intelligence},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2013.08.015},
url = {https://www.sciencedirect.com/science/article/pii/S0957417413006271},
author = {Grigori Sidorov and Francisco Velasquez and Efstathios Stamatatos and Alexander Gelbukh and Liliana Chanona-Hern√°ndez},
keywords = {Syntactic n-grams, sn-Grams, Parsing, Classification features, Syntactic paths, Authorship attribution, SVM, NB, J48},
abstract = {In this paper we introduce and discuss a concept of syntactic n-grams (sn-grams). Sn-grams differ from traditional n-grams in the manner how we construct them, i.e., what elements are considered neighbors. In case of sn-grams, the neighbors are taken by following syntactic relations in syntactic trees, and not by taking words as they appear in a text, i.e., sn-grams are constructed by following paths in syntactic trees. In this manner, sn-grams allow bringing syntactic knowledge into machine learning methods; still, previous parsing is necessary for their construction. Sn-grams can be applied in any natural language processing (NLP) task where traditional n-grams are used. We describe how sn-grams were applied to authorship attribution. We used as baseline traditional n-grams of words, part of speech (POS) tags and characters; three classifiers were applied: support vector machines (SVM), naive Bayes (NB), and tree classifier J48. Sn-grams give better results with SVM classifier.}
}
@INPROCEEDINGS{phishing,

  author={Peng, Tianrui and Harris, Ian and Sawa, Yuki},

  booktitle={2018 IEEE 12th International Conference on Semantic Computing (ICSC)}, 

  title={Detecting Phishing Attacks Using Natural Language Processing and Machine Learning}, 

  year={2018},

  volume={},

  number={},

  pages={300-301},

  doi={10.1109/ICSC.2018.00056}}
@inproceedings{humordetection,
    title = "{H}ow Did This Get Funded?! {A}utomatically Identifying Quirky Scientific Achievements",
    author = "Shani, Chen  and
      Borenstein, Nadav  and
      Shahaf, Dafna",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.2",
    doi = "10.18653/v1/2021.acl-long.2",
    pages = "14--28",
    abstract = "Humor is an important social phenomenon, serving complex social and psychological functions. However, despite being studied for millennia humor is computationally not well understood, often considered an AI-complete problem. In this work, we introduce a novel setting in humor mining: automatically detecting funny and unusual scientific papers. We are inspired by the Ig Nobel prize, a satirical prize awarded annually to celebrate funny scientific achievements (example past winner: {``}Are cows more likely to lie down the longer they stand?{''}). This challenging task has unique characteristics that make it particularly suitable for automatic learning. We construct a dataset containing thousands of funny papers and use it to learn classifiers, combining findings from psychology and linguistics with recent advances in NLP. We use our models to identify potentially funny papers in a large dataset of over 630,000 articles. The results demonstrate the potential of our methods, and more broadly the utility of integrating state-of-the-art NLP methods with insights from more traditional disciplines",
}
@inproceedings{hatespeech,
    title = "{H}ate{C}heck: Functional Tests for Hate Speech Detection Models",
    author = {R{\"o}ttger, Paul  and
      Vidgen, Bertie  and
      Nguyen, Dong  and
      Waseem, Zeerak  and
      Margetts, Helen  and
      Pierrehumbert, Janet},
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.4",
    doi = "10.18653/v1/2021.acl-long.4",
    pages = "41--58",
    abstract = "Detecting online hate is a difficult task that even state-of-the-art models struggle with. Typically, hate speech detection models are evaluated by measuring their performance on held-out test data using metrics such as accuracy and F1 score. However, this approach makes it difficult to identify specific model weak points. It also risks overestimating generalisable model performance due to increasingly well-evidenced systematic gaps and biases in hate speech datasets. To enable more targeted diagnostic insights, we introduce HateCheck, a suite of functional tests for hate speech detection models. We specify 29 model functionalities motivated by a review of previous research and a series of interviews with civil society stakeholders. We craft test cases for each functionality and validate their quality through a structured annotation process. To illustrate HateCheck{'}s utility, we test near-state-of-the-art transformer models as well as two popular commercial models, revealing critical model weaknesses.",
}
@inproceedings{schizoReddit,
    title = "Linguistic Analysis of Schizophrenia in {R}eddit Posts",
    author = "Zomick, Jonathan  and
      Levitan, Sarah Ita  and
      Serper, Mark",
    booktitle = "Proceedings of the Sixth Workshop on Computational Linguistics and Clinical Psychology",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-3009",
    doi = "10.18653/v1/W19-3009",
    pages = "74--83",
    abstract = "We explore linguistic indicators of schizophrenia in Reddit discussion forums. Schizophrenia (SZ) is a chronic mental disorder that affects a person{'}s thoughts and behaviors. Identifying and detecting signs of SZ is difficult given that SZ is relatively uncommon, affecting approximately 1{\%} of the US population, and people suffering with SZ often believe that they do not have the disorder. Linguistic abnormalities are a hallmark of SZ and many of the illness{'}s symptoms are manifested through language. In this paper we leverage the vast amount of data available from social media and use statistical and machine learning approaches to study linguistic characteristics of SZ. We collected and analyzed a large corpus of Reddit posts from users claiming to have received a formal diagnosis of SZ and identified several linguistic features that differentiated these users from a control (CTL) group. We compared these results to other findings on social media linguistic analysis and SZ. We also developed a machine learning classifier to automatically identify self-identified users with SZ on Reddit.",
}
@inproceedings{transferLearn,
    title = "Transfer Learning in Natural Language Processing",
    author = "Ruder, Sebastian  and
      Peters, Matthew E.  and
      Swayamdipta, Swabha  and
      Wolf, Thomas",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Tutorials",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-5004",
    doi = "10.18653/v1/N19-5004",
    pages = "15--18",
    abstract = "The classic supervised machine learning paradigm is based on learning in isolation, a single predictive model for a task using a single dataset. This approach requires a large number of training examples and performs best for well-defined and narrow tasks. Transfer learning refers to a set of methods that extend this approach by leveraging data from additional domains or tasks to train a model with better generalization properties. Over the last two years, the field of Natural Language Processing (NLP) has witnessed the emergence of several transfer learning methods and architectures which significantly improved upon the state-of-the-art on a wide range of NLP tasks. These improvements together with the wide availability and ease of integration of these methods are reminiscent of the factors that led to the success of pretrained word embeddings and ImageNet pretraining in computer vision, and indicate that these methods will likely become a common tool in the NLP landscape as well as an important research direction. We will present an overview of modern transfer learning methods in NLP, how models are pre-trained, what information the representations they learn capture, and review examples and case studies on how these models can be integrated and adapted in downstream NLP tasks.",
}
@article{sarcasm,
    title = "Sarcasm Analysis Using Conversation Context",
    author = "Ghosh, Debanjan  and
      Fabbri, Alexander R.  and
      Muresan, Smaranda",
    journal = "Computational Linguistics",
    volume = "44",
    number = "4",
    month = dec,
    year = "2018",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/J18-4009",
    doi = "10.1162/coli_a_00336",
    pages = "755--792",
    abstract = "Computational models for sarcasm detection have often relied on the content of utterances in isolation. However, the speaker{'}s sarcastic intent is not always apparent without additional context. Focusing on social media discussions, we investigate three issues: (1) does modeling conversation context help in sarcasm detection? (2) can we identify what part of conversation context triggered the sarcastic reply? and (3) given a sarcastic post that contains multiple sentences, can we identify the specific sentence that is sarcastic? To address the first issue, we investigate several types of Long Short-Term Memory (LSTM) networks that can model both the conversation context and the current turn. We show that LSTM networks with sentence-level attention on context and current turn, as well as the conditional LSTM network, outperform the LSTM model that reads only the current turn. As conversation context, we consider the prior turn, the succeeding turn, or both. Our computational models are tested on two types of social media platforms: Twitter and discussion forums. We discuss several differences between these data sets, ranging from their size to the nature of the gold-label annotations. To address the latter two issues, we present a qualitative analysis of the attention weights produced by the LSTM models (with attention) and discuss the results compared with human performance on the two tasks.",
}
@article{irony,
    title = "We Usually Don{'}t Like Going to the Dentist: Using Common Sense to Detect Irony on {T}witter",
    author = "Van Hee, Cynthia  and
      Lefever, Els  and
      Hoste, V{\'e}ronique",
    journal = "Computational Linguistics",
    volume = "44",
    number = "4",
    month = dec,
    year = "2018",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/J18-4010",
    doi = "10.1162/coli_a_00337",
    pages = "793--832",
    abstract = "Although common sense and connotative knowledge come naturally to most people, computers still struggle to perform well on tasks for which such extratextual information is required. Automatic approaches to sentiment analysis and irony detection have revealed that the lack of such world knowledge undermines classification performance. In this article, we therefore address the challenge of modeling implicit or prototypical sentiment in the framework of automatic irony detection. Starting from manually annotated connoted situation phrases (e.g., {``}flight delays,{''} {``}sitting the whole day at the doctor{'}s office{''}), we defined the implicit sentiment held towards such situations automatically by using both a lexico-semantic knowledge base and a data-driven method. We further investigate how such implicit sentiment information affects irony detection by assessing a state-of-the-art irony classifier before and after it is informed with implicit sentiment information.",
}
@inproceedings{unsupervisedClass,
    title = "Towards Unsupervised Text Classification Leveraging Experts and Word Embeddings",
    author = "Haj-Yahia, Zied  and
      Sieg, Adrien  and
      Deleris, L{\'e}a A.",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1036",
    doi = "10.18653/v1/P19-1036",
    pages = "371--379",
    abstract = "Text classification aims at mapping documents into a set of predefined categories. Supervised machine learning models have shown great success in this area but they require a large number of labeled documents to reach adequate accuracy. This is particularly true when the number of target categories is in the tens or the hundreds. In this work, we explore an unsupervised approach to classify documents into categories simply described by a label. The proposed method is inspired by the way a human proceeds in this situation: It draws on textual similarity between the most relevant words in each document and a dictionary of keywords for each category reflecting its semantics and lexical field. The novelty of our method hinges on the enrichment of the category labels through a combination of human expertise and language models, both generic and domain specific. Our experiments on 5 standard corpora show that the proposed method increases F1-score over relying solely on human expertise and can also be on par with simple supervised approaches. It thus provides a practical alternative to situations where low cost text categorization is needed, as we illustrate with our application to operational risk incidents classification.",
}
@article{neuralText,
    author = {Zhang, Hao and Sproat, Richard and Ng, Axel H. and Stahlberg, Felix and Peng, Xiaochang and Gorman, Kyle and Roark, Brian},
    title = "{Neural Models of Text Normalization for Speech Applications}",
    journal = {Computational Linguistics},
    volume = {45},
    number = {2},
    pages = {293-337},
    year = {2019},
    month = {06},
    abstract = "{Machine learning, including neural network techniques, have been applied to virtually every domain in natural language processing. One problem that has been somewhat resistant to effective machine learning solutions is text normalization for speech applications such as text-to-speech synthesis (TTS). In this application, one must decide, for example, that 123 is verbalized as one hundred twenty three in 123 pages but as one twenty three in 123 King Ave. For this task, state-of-the-art industrial systems depend heavily on hand-written language-specific grammars.We propose neural network models that treat text normalization for TTS as a sequence-to-sequence problem, in which the input is a text token in context, and the output is the verbalization of that token. We find that the most effective model, in accuracy and efficiency, is one where the sentential context is computed once and the results of that computation are combined with the computation of each token in sequence to compute the verbalization. This model allows for a great deal of flexibility in terms of representing the context, and also allows us to integrate tagging and segmentation into the process.These models perform very well overall, but occasionally they will predict wildly inappropriate verbalizations, such as reading 3 cm as three kilometers. Although rare, such verbalizations are a major issue for TTS applications. We thus use finite-state covering grammars to guide the neural models, either during training and decoding, or just during decoding, away from such ‚Äúunrecoverable‚Äù errors. Such grammars can largely be learned from data.}",
    issn = {0891-2017},
    doi = {10.1162/coli_a_00349},
    url = {https://doi.org/10.1162/coli\_a\_00349},
    eprint = {https://direct.mit.edu/coli/article-pdf/45/2/293/1809815/coli\_a\_00349.pdf},
}
@inproceedings{politicalHate,
    title = "Impact of Politically Biased Data on Hate Speech Classification",
    author = "Wich, Maximilian  and
      Bauer, Jan  and
      Groh, Georg",
    booktitle = "Proceedings of the Fourth Workshop on Online Abuse and Harms",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.alw-1.7",
    doi = "10.18653/v1/2020.alw-1.7",
    pages = "54--64",
    abstract = "One challenge that social media platforms are facing nowadays is hate speech. Hence, automatic hate speech detection has been increasingly researched in recent years - in particular with the rise of deep learning. A problem of these models is their vulnerability to undesirable bias in training data. We investigate the impact of political bias on hate speech classification by constructing three politically-biased data sets (left-wing, right-wing, politically neutral) and compare the performance of classifiers trained on them. We show that (1) political bias negatively impairs the performance of hate speech classifiers and (2) an explainable machine learning model can help to visualize such bias within the training data. The results show that political bias in training data has an impact on hate speech classification and can become a serious issue.",
}
@inproceedings{commonsense,
    title = "Commonsense Knowledge Mining from Pretrained Models",
    author = "Davison, Joe  and
      Feldman, Joshua  and
      Rush, Alexander",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1109",
    doi = "10.18653/v1/D19-1109",
    pages = "1173--1178",
    abstract = "Inferring commonsense knowledge is a key challenge in machine learning. Due to the sparsity of training data, previous work has shown that supervised methods for commonsense knowledge mining underperform when evaluated on novel data. In this work, we develop a method for generating commonsense knowledge using a large, pre-trained bidirectional language model. By transforming relational triples into masked sentences, we can use this model to rank a triple{'}s validity by the estimated pointwise mutual information between the two entities. Since we do not update the weights of the bidirectional model, our approach is not biased by the coverage of any one commonsense knowledge base. Though we do worse on a held-out test set than models explicitly trained on a corresponding training set, our approach outperforms these methods when mining commonsense knowledge from new sources, suggesting that our unsupervised technique generalizes better than current supervised approaches.",
}
@inproceedings{eventExtract,
    title = "Event Extraction as Machine Reading Comprehension",
    author = "Liu, Jian  and
      Chen, Yubo  and
      Liu, Kang  and
      Bi, Wei  and
      Liu, Xiaojiang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.128",
    doi = "10.18653/v1/2020.emnlp-main.128",
    pages = "1641--1651",
    abstract = "Event extraction (EE) is a crucial information extraction task that aims to extract event information in texts. Previous methods for EE typically model it as a classification task, which are usually prone to the data scarcity problem. In this paper, we propose a new learning paradigm of EE, by explicitly casting it as a machine reading comprehension problem (MRC). Our approach includes an unsupervised question generation process, which can transfer event schema into a set of natural questions, followed by a BERT-based question-answering process to retrieve answers as EE results. This learning paradigm enables us to strengthen the reasoning process of EE, by introducing sophisticated models in MRC, and relieve the data scarcity problem, by introducing the large-scale datasets in MRC. The empirical results show that: i) our approach attains state-of-the-art performance by considerable margins over previous methods. ii) Our model is excelled in the data-scarce scenario, for example, obtaining 49.8{\%} in F1 for event argument extraction with only 1{\%} data, compared with 2.2{\%} of the previous method. iii) Our model also fits with zero-shot scenarios, achieving 37.0{\%} and 16{\%} in F1 on two datasets without using any EE training data.",
}
@inproceedings{selfSuper,
    title = "Self-Supervised Neural Machine Translation",
    author = "Ruiter, Dana  and
      Espa{\~n}a-Bonet, Cristina  and
      van Genabith, Josef",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1178",
    doi = "10.18653/v1/P19-1178",
    pages = "1828--1834",
    abstract = "We present a simple new method where an emergent NMT system is used for simultaneously selecting training data and learning internal NMT representations. This is done in a self-supervised way without parallel data, in such a way that both tasks enhance each other during training. The method is language independent, introduces no additional hyper-parameters, and achieves BLEU scores of 29.21 (en2fr) and 27.36 (fr2en) on newstest2014 using English and French Wikipedia data for training.",
}
@article{modelingAsync,
    title = "Modeling Speech Acts in Asynchronous Conversations: A Neural-{CRF} Approach",
    author = "Joty, Shafiq  and
      Mohiuddin, Tasnim",
    journal = "Computational Linguistics",
    volume = "44",
    number = "4",
    month = dec,
    year = "2018",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/J18-4012",
    doi = "10.1162/coli_a_00339",
    pages = "859--894",
    abstract = "Participants in an asynchronous conversation (e.g., forum, e-mail) interact with each other at different times, performing certain communicative acts, called speech acts (e.g., question, request). In this article, we propose a hybrid approach to speech act recognition in asynchronous conversations. Our approach works in two main steps: a long short-term memory recurrent neural network (LSTM-RNN) first encodes each sentence separately into a task-specific distributed representation, and this is then used in a conditional random field (CRF) model to capture the conversational dependencies between sentences. The LSTM-RNN model uses pretrained word embeddings learned from a large conversational corpus and is trained to classify sentences into speech act types. The CRF model can consider arbitrary graph structures to model conversational dependencies in an asynchronous conversation. In addition, to mitigate the problem of limited annotated data in the asynchronous domains, we adapt the LSTM-RNN model to learn from synchronous conversations (e.g., meetings), using domain adversarial training of neural networks. Empirical evaluation shows the effectiveness of our approach over existing ones: (i) LSTM-RNNs provide better task-specific representations, (ii) conversational word embeddings benefit the LSTM-RNNs more than the off-the-shelf ones, (iii) adversarial training gives better domain-invariant representations, and (iv) the global CRF model improves over local models.",
}
@inproceedings{wordEmbed,
    title = "Word Embeddings, Analogies, and Machine Learning: Beyond king - man + woman = queen",
    author = "Drozd, Aleksandr  and
      Gladkova, Anna  and
      Matsuoka, Satoshi",
    booktitle = "Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",
    month = dec,
    year = "2016",
    address = "Osaka, Japan",
    publisher = "The COLING 2016 Organizing Committee",
    url = "https://aclanthology.org/C16-1332",
    pages = "3519--3530",
    abstract = "Solving word analogies became one of the most popular benchmarks for word embeddings on the assumption that linear relations between word pairs (such as \textit{king}:\textit{man} :: \textit{woman}:\textit{queen}) are indicative of the quality of the embedding. We question this assumption by showing that the information not detected by linear offset may still be recoverable by a more sophisticated search method, and thus is actually encoded in the embedding. The general problem with linear offset is its sensitivity to the idiosyncrasies of individual words. We show that simple averaging over multiple word pairs improves over the state-of-the-art. A further improvement in accuracy (up to 30{\%} for some embeddings and relations) is achieved by combining cosine similarity with an estimation of the extent to which a candidate answer belongs to the correct word class. In addition to this practical contribution, this work highlights the problem of the interaction between word embeddings and analogy retrieval algorithms, and its implications for the evaluation of word embeddings and the use of analogies in extrinsic tasks.",
}
@inproceedings{catasFor,
    title = "Overcoming Catastrophic Forgetting During Domain Adaptation of Neural Machine Translation",
    author = "Thompson, Brian  and
      Gwinnup, Jeremy  and
      Khayrallah, Huda  and
      Duh, Kevin  and
      Koehn, Philipp",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1209",
    doi = "10.18653/v1/N19-1209",
    pages = "2062--2068",
    abstract = "Continued training is an effective method for domain adaptation in neural machine translation. However, in-domain gains from adaptation come at the expense of general-domain performance. In this work, we interpret the drop in general-domain performance as catastrophic forgetting of general-domain knowledge. To mitigate it, we adapt Elastic Weight Consolidation (EWC){---}a machine learning method for learning a new task without forgetting previous tasks. Our method retains the majority of general-domain performance lost in continued training without degrading in-domain performance, outperforming the previous state-of-the-art. We also explore the full range of general-domain performance available when some in-domain degradation is acceptable.",
}
@inproceedings{harassClass,
    title = "A Novel Methodology for Developing Automatic Harassment Classifiers for {T}witter",
    author = "Arora, Ishaan  and
      Guo, Julia  and
      Levitan, Sarah Ita  and
      McGregor, Susan  and
      Hirschberg, Julia",
    booktitle = "Proceedings of the Fourth Workshop on Online Abuse and Harms",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.alw-1.2",
    doi = "10.18653/v1/2020.alw-1.2",
    pages = "7--15",
    abstract = "Most efforts at identifying abusive speech online rely on public corpora that have been scraped from websites using keyword-based queries or released by site or platform owners for research purposes. These are typically labeled by crowd-sourced annotators {--} not the targets of the abuse themselves. While this method of data collection supports fast development of machine learning classifiers, the models built on them often fail in the context of real-world harassment and abuse, which contain nuances less easily identified by non-targets. Here, we present a mixed-methods approach to create classifiers for abuse and harassment which leverages direct engagement with the target group in order to achieve high quality and ecological validity of data sets and labels, and to generate deeper insights into the key tactics of bad actors. We use women journalists{'} experience on Twitter as an initial community of focus. We identify several structural mechanisms of abuse that we believe will generalize to other target communities.",
}
@misc{twoStep,
  doi = {10.48550/ARXIV.1706.01206},
  
  url = {https://arxiv.org/abs/1706.01206},
  
  author = {Park, Ji Ho and Fung, Pascale},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {One-step and Two-step Classification for Abusive Language Detection on Twitter},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@misc{raceBi,
  doi = {10.48550/ARXIV.1905.12516},
  
  url = {https://arxiv.org/abs/1905.12516},
  
  author = {Davidson, Thomas and Bhattacharya, Debasmita and Weber, Ingmar},
  
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Racial Bias in Hate Speech and Abusive Language Detection Datasets},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@inproceedings{biasData,
    title = "{D}etection of {A}busive {L}anguage: the {P}roblem of {B}iased {D}atasets",
    author = "Wiegand, Michael  and
      Ruppenhofer, Josef  and
      Kleinbauer, Thomas",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1060",
    doi = "10.18653/v1/N19-1060",
    pages = "602--608",
    abstract = "We discuss the impact of data bias on abusive language detection. We show that classification scores on popular datasets reported in previous work are much lower under realistic settings in which this bias is reduced. Such biases are most notably observed on datasets that are created by focused sampling instead of random sampling. Datasets with a higher proportion of implicit abuse are more affected than datasets with a lower proportion.",
}
@inproceedings{abusiveLang,
author = {Nobata, Chikashi and Tetreault, Joel and Thomas, Achint and Mehdad, Yashar and Chang, Yi},
title = {Abusive Language Detection in Online User Content},
year = {2016},
isbn = {9781450341431},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872427.2883062},
doi = {10.1145/2872427.2883062},
abstract = {Detection of abusive language in user generated online content has become an issue of increasing importance in recent years. Most current commercial methods make use of blacklists and regular expressions, however these measures fall short when contending with more subtle, less ham-fisted examples of hate speech. In this work, we develop a machine learning based method to detect hate speech on online user comments from two domains which outperforms a state-of-the-art deep learning approach. We also develop a corpus of user comments annotated for abusive language, the first of its kind. Finally, we use our detection tool to analyze abusive language over time and in different settings to further enhance our knowledge of this behavior.},
booktitle = {Proceedings of the 25th International Conference on World Wide Web},
pages = {145‚Äì153},
numpages = {9},
keywords = {nlp, hate speech, stylistic classification, abusive language, natural language processing, discourse classification},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16}
}
@inproceedings{classModel,
    title = "Class-based Prediction Errors to Detect Hate Speech with Out-of-vocabulary Words",
    author = "Serr{\`a}, Joan  and
      Leontiadis, Ilias  and
      Spathis, Dimitris  and
      Stringhini, Gianluca  and
      Blackburn, Jeremy  and
      Vakali, Athena",
    booktitle = "Proceedings of the First Workshop on Abusive Language Online",
    month = aug,
    year = "2017",
    address = "Vancouver, BC, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W17-3005",
    doi = "10.18653/v1/W17-3005",
    pages = "36--40",
    abstract = "Common approaches to text categorization essentially rely either on n-gram counts or on word embeddings. This presents important difficulties in highly dynamic or quickly-interacting environments, where the appearance of new words and/or varied misspellings is the norm. A paradigmatic example of this situation is abusive online behavior, with social networks and media platforms struggling to effectively combat uncommon or non-blacklisted hate words. To better deal with these issues in those fast-paced environments, we propose using the error signal of class-based language models as input to text classification algorithms. In particular, we train a next-character prediction model for any given class and then exploit the error of such class-based models to inform a neural network classifier. This way, we shift from the {`}ability to describe{'} seen documents to the {`}ability to predict{'} unseen content. Preliminary studies using out-of-vocabulary splits from abusive tweet data show promising results, outperforming competitive text categorization strategies by 4-11{\%}.",
}
@inproceedings{annoInf,
  title={Are you a racist or am i seeing things? annotator influence on hate speech detection on twitter},
  author={Waseem, Zeerak},
  booktitle={Proceedings of the first workshop on NLP and computational social science},
  pages={138--142},
  year={2016}
}
@inproceedings{lexiAbu,
    title = "Inducing a Lexicon of Abusive Words {--} a Feature-Based Approach",
    author = "Wiegand, Michael  and
      Ruppenhofer, Josef  and
      Schmidt, Anna  and
      Greenberg, Clayton",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-1095",
    doi = "10.18653/v1/N18-1095",
    pages = "1046--1056",
    abstract = "We address the detection of abusive words. The task is to identify such words among a set of negative polar expressions. We propose novel features employing information from both corpora and lexical resources. These features are calibrated on a small manually annotated base lexicon which we use to produce a large lexicon. We show that the word-level information we learn cannot be equally derived from a large dataset of annotated microposts. We demonstrate the effectiveness of our (domain-independent) lexicon in the cross-domain detection of abusive microposts.",
}

