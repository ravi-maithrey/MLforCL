{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "racism_train = 1528\n",
    "racism_dev = 191\n",
    "racism_test = 191\n",
    "\n",
    "sexism_train = 2437\n",
    "sexism_dev = 305\n",
    "sexism_test = 304\n",
    "\n",
    "none_train = 8605\n",
    "none_dev = 1076\n",
    "none_test = 1075"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fVector(dataset, lexicon):\n",
    "    featureVector = [] \n",
    "    finalData = []\n",
    "    for _, tweet in dataset:\n",
    "        for word in lexicon:\n",
    "            if word in tweet:\n",
    "                featureVector.append(1)\n",
    "            else:\n",
    "               featureVector.append(0)\n",
    "        finalData.append(featureVector)\n",
    "    return finalData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "none = pd.read_csv(\"none.csv\", header=None)\n",
    "sexism = pd.read_csv(\"sexism.csv\", header=None)\n",
    "racism = pd.read_csv(\"racism.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "none = none.fillna('ExtraWord')\n",
    "sexism = sexism.fillna('ExtraWord')\n",
    "racism = racism.fillna('ExtraWord')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon = pd.read_csv(\"lexicon_mod.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features(file_path, lexicon_path):\n",
    "    feature_matrix = []\n",
    "    # lexicon = open(lexicon_path)\n",
    "    lexicon_words = [line.rstrip() for line in open(lexicon_path)]\n",
    "    file = open(file_path)\n",
    "    while True:\n",
    "        tweet = file.readline()\n",
    "        split = tweet.split(\",\")\n",
    "        words = split\n",
    "        # words = [w for w in split if w]\n",
    "        feature_vector = []\n",
    "        for l_word in lexicon_words:\n",
    "            if l_word in words:\n",
    "                feature_vector.append(1)\n",
    "            else:\n",
    "                feature_vector.append(0)\n",
    "        feature_matrix.append(feature_vector)\n",
    "        if not tweet:\n",
    "            break\n",
    "    return feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "none_features = features(\"none.csv\", \"hate_lexicon_wiegand.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sexism_features = features(\"sexism.csv\", \"hate_lexicon_wiegand.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "racism_features = features(\"racism.csv\", \"hate_lexicon_wiegand.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"DO NOT RUN THIS\"\"\"\n",
    "append_flag = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "if append_flag == 0:\n",
    "    append_flag = 1\n",
    "    for fv in none_features:\n",
    "        fv.append(0)\n",
    "    for fv in sexism_features:\n",
    "        fv.append(1)\n",
    "    for fv in racism_features:\n",
    "        fv.append(2)\n",
    "else:\n",
    "    print(\"You've already appended the labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "racism_f_train = racism_features[0:1528]\n",
    "racism_f_dev = racism_features[1528:(1528+191)]\n",
    "racism_f_test = racism_features[(1528+191):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "sexism_f_train = sexism_features[0:2473]\n",
    "sexism_f_dev = sexism_features[2473:(2473+305)]\n",
    "sexism_f_test = sexism_features[(2473+305):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "none_f_train = none_features[0:8605]\n",
    "none_f_dev = none_features[8605:(8605+1076)]\n",
    "none_f_test = none_features[(8605+1076):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "racism_f_train = np.array(racism_f_train)\n",
    "racism_f_dev = np.array(racism_f_dev)\n",
    "racism_f_test = np.array(racism_f_test)\n",
    "\n",
    "sexism_f_test = np.array(sexism_f_test)\n",
    "sexism_f_train = np.array(sexism_f_train)\n",
    "sexism_f_dev = np.array(sexism_f_dev)\n",
    "\n",
    "none_f_train = np.array(none_f_train)\n",
    "none_f_dev = np.array(none_f_dev)\n",
    "none_f_test = np.array(none_f_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "mid_train = np.concatenate((racism_f_train, sexism_f_train))\n",
    "train = np.concatenate((mid_train, none_f_train))\n",
    "\n",
    "mid_dev = np.concatenate((racism_f_dev, sexism_f_dev))\n",
    "dev = np.concatenate((mid_dev, none_f_dev))\n",
    "\n",
    "mid_test = np.concatenate((racism_f_test, sexism_f_test))\n",
    "test = np.concatenate((mid_test, none_f_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(train)\n",
    "np.random.shuffle(test)\n",
    "np.random.shuffle(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_df = pd.DataFrame(train)\n",
    "te_df = pd.DataFrame(test)\n",
    "dv_df = pd.DataFrame(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_df.to_csv(\"data.train\", header=None)\n",
    "te_df.to_csv(\"data.test\", header=None)\n",
    "dv_df.to_csv(\"data.dev\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"data.train\", train, fmt=\"%i\",  delimiter=',')\n",
    "np.savetxt(\"data.dev\", dev,  fmt=\"%i\", delimiter=\",\")\n",
    "np.savetxt(\"data.test\", test,  fmt=\"%i\",delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "51d0634a2487aff1c7d1f3b0822ad0679aa27b729b0ed920cf0f286e3bcfded9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
